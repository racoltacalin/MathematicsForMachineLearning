{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, vectors are special objects that can be added together and miltiplied by scalars to produce another object of the same king!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Geometric vectors\n",
    "#### 1.2. Polynomials are also vectors\n",
    "#### 1.3. Audio signals are vectors\n",
    "#### 1.4. Elements of R^n (tuples of n real numbers) are vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of a vector space and its properties underline much of ***Machine Learning***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INFO 1:* Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables.**\n",
    "\n",
    "***INFO 2:* Groups play an important role in computer science.Besides providing a fundamental framework for operations on sets, they are heavily used in\n",
    "cryptography, coding theory, and graphics.**\n",
    "\n",
    "***INFO 3:* Vector subspaces are a key idea in machine learning.**\n",
    "\n",
    "***INFO 4:* Linear independence is one of the most important concepts in linear\n",
    "algebra.**\n",
    "\n",
    "***INFO 5:* In the machine learning literature, the distinction between linear and affine is sometimes not clear so that we can find references to affine spaces/mappings as linear spaces/mappings.**\n",
    "\n",
    "***INFO 6:* Symmetric, positive definite matrices play an important role in machine learning, and they are defined via the inner product. The idea of symmetric positive semidefinite matrices is key in the definition of kernels**\n",
    "\n",
    "***INFO 7:* Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally find the most informative dimensions in the data.**\n",
    "\n",
    "***INFO 8:* In machine learning, inner products are important in the context of kernel methods.**\n",
    "\n",
    "***INFO 9:* Methods to analyze and learn from network data are an essential component of machine learning methods.**\n",
    "\n",
    "***INFO 10:* The Cholesky decomposition is an important tool for the numerical computations underlying machine learning.**\n",
    "\n",
    "***INFO 11:* The SVD is used in a variety of applications in machine learning from least-squares problems in curve fitting to solving systems of linear equations. Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical rounding errors. The SVD’s ability to approximate matrices with “simpler” matrices in a principled manner opens up machine learning applications ranging from dimensionality reduction and topic modeling to data compression and clustering.**\n",
    "\n",
    "***INFO 12:* The low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems.**\n",
    "\n",
    "***INFO 13:* Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model explains the data: Finding good parameters can be phrased as an optimization problem.**\n",
    "\n",
    "***INFO 14:* Vector calculus is one of the fundamental mathematical tools we need in machine learning.**\n",
    "\n",
    "***INFO 15:* To facilitate learning in machine learning models, we need to compute gradients of functions, since the gradient points in the direction of steepest ascent.**\n",
    "\n",
    "***INFO 16:* When we compute gradients and implement them, we can use finite differences to numerically test our computation and implementation: We choose the value 'h' to be small (e.g. h = 10^-4) and compare the finite-difference approximation with our -analytic- implementation of the gradient. If the error is small, our gradient implementation is probably correct.**\n",
    "\n",
    "***INFO 17:* The Jacobian determinant is important because we transform random variables and probability distributions. These transformations are extremely relevant in machine learning in the context of *training deep neural networks* using the\n",
    "reparametrization trick, also called infinite perturbation analysis.**\n",
    "\n",
    "***INFO 18:* In many machine learning applications, we find good model parameters by performing gradient descent which relies on the fact that we can compute the gradient of a learning objective with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 19:* For training deep neural network models, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986) is an efficient way to compute the gradient of an error function with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 20:* An area where the chain rule is used to an extreme is deep learning, where the function value y is computed as a many-level function composition.**\n",
    "\n",
    "***INFO 21:* It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation.**\n",
    "\n",
    "***INFO 22:* In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of the labels, the reverse mode is computationally significantly cheaper than the forward mode.**\n",
    "\n",
    "***INFO 23:* For neural network training, we backpropagate the error of the prediction with respect to the label.**\n",
    "\n",
    "***INFO 24:* In machine learning (and other disciplines), we often need to compute expectations, i.e., we need to solve integrals.**\n",
    "\n",
    "***INFO 25:* In machine learning and statistics, there are two major interpretations of probability: the Bayesian and frequentist interpretations.**\n",
    "\n",
    "***INFO 26:* In machine learning, we often avoid explicitly referring to the probability space, but instead refer to probabilities on quantities of interest, which we denote by T. We refer to T as the target space and refer to elements of T as states.**\n",
    "\n",
    "***INFO 27:* In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-fitting” model for some\n",
    "data.**\n",
    "\n",
    "***INFO 28:* Another aspect of machine learning systems is that we are interested in generalization error. This means that we are actually interested in the performance of our system on instances that we will observe in future, which are not identical to the instances that we have seen so far.**\n",
    "\n",
    "***INFO 29:* In machine learning, we use discrete probability distributions to model categorical variables, i.e., variables that take a finite set of unordered values. They could be categorical features, such as the degree taken at university when used for predicting the salary of a person, or categorical labels, such as letters of the alphabet when doing handwriting recognition.**\n",
    "\n",
    "***INFO 30:* However, in many machine learning applications discrete states take numerical values, e.g., z1 = -1.1; z2 = 0.3; z3 = 1.5, where we could say z1 < z2 < z3. Discrete states that assume numerical values are particularly useful because we often consider expected values of random variables.**\n",
    "\n",
    "***INFO 31:* Unfortunately, machine learning literature uses notation and nomenclature that hides the distinction between the sample space &#x2126; , the target space T , and the random variable X.** \n",
    "\n",
    "***INFO 32:* In line with most machine learning literature, we also rely on context to distinguish the different uses of the phrase probability distribution.**\n",
    "\n",
    "***INFO 33:* Probabilistic modeling provides a principled foundation for designing machine learning methods.**\n",
    "\n",
    "***INFO 34:* In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent)\\ random variables given that we have observed other random variables.**\n",
    "\n",
    "***INFO 35:* If we think in a bigger context, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances.**\n",
    "\n",
    "***INFO 36:* For example, in the context of model-based reinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures. Therefore, having the full posterior can be very useful for a downstream task.**\n",
    "\n",
    "***INFO 37:* The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000).**\n",
    "\n",
    "***INFO 38:* In machine learning, we need to learn from empirical observations of data.**\n",
    "\n",
    "***INFO 39:* We use the empirical covariance, which is a biased estimate. The unbiased (sometimes called corrected) covariance has the factor N-1 in the denominator instead of N.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
