{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, vectors are special objects that can be added together and miltiplied by scalars to produce another object of the same king!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Geometric vectors\n",
    "#### 1.2. Polynomials are also vectors\n",
    "#### 1.3. Audio signals are vectors\n",
    "#### 1.4. Elements of R^n (tuples of n real numbers) are vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of a vector space and its properties underline much of ***Machine Learning***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INFO 1:* Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables.**\n",
    "\n",
    "***INFO 2:* Groups play an important role in computer science.Besides providing a fundamental framework for operations on sets, they are heavily used in\n",
    "cryptography, coding theory, and graphics.**\n",
    "\n",
    "***INFO 3:* Vector subspaces are a key idea in machine learning.**\n",
    "\n",
    "***INFO 4:* Linear independence is one of the most important concepts in linear\n",
    "algebra.**\n",
    "\n",
    "***INFO 5:* In the machine learning literature, the distinction between linear and affine is sometimes not clear so that we can find references to affine spaces/mappings as linear spaces/mappings.**\n",
    "\n",
    "***INFO 6:* Symmetric, positive definite matrices play an important role in machine learning, and they are defined via the inner product. The idea of symmetric positive semidefinite matrices is key in the definition of kernels**\n",
    "\n",
    "***INFO 7:* Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally find the most informative dimensions in the data.**\n",
    "\n",
    "***INFO 8:* In machine learning, inner products are important in the context of kernel methods.**\n",
    "\n",
    "***INFO 9:* Methods to analyze and learn from network data are an essential component of machine learning methods.**\n",
    "\n",
    "***INFO 10:* The Cholesky decomposition is an important tool for the numerical computations underlying machine learning.**\n",
    "\n",
    "***INFO 11:* The SVD is used in a variety of applications in machine learning from least-squares problems in curve fitting to solving systems of linear equations. Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical rounding errors. The SVD’s ability to approximate matrices with “simpler” matrices in a principled manner opens up machine learning applications ranging from dimensionality reduction and topic modeling to data compression and clustering.**\n",
    "\n",
    "***INFO 12:* The low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems.**\n",
    "\n",
    "***INFO 13:* Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model explains the data: Finding good parameters can be phrased as an optimization problem.**\n",
    "\n",
    "***INFO 14:* Vector calculus is one of the fundamental mathematical tools we need in machine learning.**\n",
    "\n",
    "***INFO 15:* To facilitate learning in machine learning models, we need to compute gradients of functions, since the gradient points in the direction of steepest ascent.**\n",
    "\n",
    "***INFO 16:* When we compute gradients and implement them, we can use finite differences to numerically test our computation and implementation: We choose the value 'h' to be small (e.g. h = 10^-4) and compare the finite-difference approximation with our -analytic- implementation of the gradient. If the error is small, our gradient implementation is probably correct.**\n",
    "\n",
    "***INFO 17:* The Jacobian determinant is important because we transform random variables and probability distributions. These transformations are extremely relevant in machine learning in the context of *training deep neural networks* using the\n",
    "reparametrization trick, also called infinite perturbation analysis.**\n",
    "\n",
    "***INFO 18:* In many machine learning applications, we find good model parameters by performing gradient descent which relies on the fact that we can compute the gradient of a learning objective with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 19:* For training deep neural network models, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986) is an efficient way to compute the gradient of an error function with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 20:* An area where the chain rule is used to an extreme is deep learning, where the function value y is computed as a many-level function composition.**\n",
    "\n",
    "***INFO 21:* It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation.**\n",
    "\n",
    "***INFO 22:* In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of the labels, the reverse mode is computationally significantly cheaper than the forward mode.**\n",
    "\n",
    "***INFO 23:* For neural network training, we backpropagate the error of the prediction with respect to the label.**\n",
    "\n",
    "***INFO 24:* In machine learning (and other disciplines), we often need to compute expectations, i.e., we need to solve integrals.**\n",
    "\n",
    "***INFO 25:* In machine learning and statistics, there are two major interpretations of probability: the Bayesian and frequentist interpretations.**\n",
    "\n",
    "***INFO 26:* In machine learning, we often avoid explicitly referring to the probability space, but instead refer to probabilities on quantities of interest, which we denote by T. We refer to T as the target space and refer to elements of T as states.**\n",
    "\n",
    "***INFO 27:* In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-fitting” model for some\n",
    "data.**\n",
    "\n",
    "***INFO 28:* Another aspect of machine learning systems is that we are interested in generalization error. This means that we are actually interested in the performance of our system on instances that we will observe in future, which are not identical to the instances that we have seen so far.**\n",
    "\n",
    "***INFO 29:* In machine learning, we use discrete probability distributions to model categorical variables, i.e., variables that take a finite set of unordered values. They could be categorical features, such as the degree taken at university when used for predicting the salary of a person, or categorical labels, such as letters of the alphabet when doing handwriting recognition.**\n",
    "\n",
    "***INFO 30:* However, in many machine learning applications discrete states take numerical values, e.g., z1 = -1.1; z2 = 0.3; z3 = 1.5, where we could say z1 < z2 < z3. Discrete states that assume numerical values are particularly useful because we often consider expected values of random variables.**\n",
    "\n",
    "***INFO 31:* Unfortunately, machine learning literature uses notation and nomenclature that hides the distinction between the sample space &#x2126; , the target space T , and the random variable X.** \n",
    "\n",
    "***INFO 32:* In line with most machine learning literature, we also rely on context to distinguish the different uses of the phrase probability distribution.**\n",
    "\n",
    "***INFO 33:* Probabilistic modeling provides a principled foundation for designing machine learning methods.**\n",
    "\n",
    "***INFO 34:* In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent)\\ random variables given that we have observed other random variables.**\n",
    "\n",
    "***INFO 35:* If we think in a bigger context, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances.**\n",
    "\n",
    "***INFO 36:* For example, in the context of model-based reinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures. Therefore, having the full posterior can be very useful for a downstream task.**\n",
    "\n",
    "***INFO 37:* The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000).**\n",
    "\n",
    "***INFO 38:* In machine learning, we need to learn from empirical observations of data.**\n",
    "\n",
    "***INFO 39:* We use the empirical covariance, which is a biased estimate. The unbiased (sometimes called corrected) covariance has the factor N-1 in the denominator instead of N.**\n",
    "\n",
    "***INFO 40:* The raw-score version of the variance can be useful in machine learning, e.g., when deriving the bias–variance decomposition (Bishop, 2006).**\n",
    "\n",
    "***INFO 41:* In machine learning, we often consider problems that can be modeled as independent and identically distributed (i.i.d.) random variables, X_1,...,X_N.**\n",
    "\n",
    "***INFO 42:* Another concept that is important in machine learning is conditional independence.**\n",
    "\n",
    "***INFO 43:* There are many other areas of machine learning that also benefit from using a Gaussian distribution, for example Gaussian processes, variational inference, and reinforcement learning.**\n",
    "\n",
    "***INFO 44:* Gaussians are widely used in statistical estimation and machine learning as they have closed-form expressions for marginal and conditional distributions.**\n",
    "\n",
    "***INFO 45:* It is worth recalling at this point the desiderata for manipulating probability distributions in the machine learning context: *1.* There is some “closure property” when applying the rules of probability, e.g., Bayes’ theorem. By closure, we mean that applying a particular operation returns an object of the same type. *2.* As we collect more data, we do not need more parameters to describe the distribution. *3.* Since we are interested in learning from data, we want parameter estimation to behave nicely.**\n",
    "\n",
    "***INFO 46:* The rewriting above of the Bernoulli distribution, where we use Boolean variables as numerical 0 or 1 and express them in the exponents, is a trick that is often used in machine learning textbooks.**\n",
    "\n",
    "***INFO 47:* In machine learning, we consider a finite number of samples from a distribution. One could imagine that for simple distributions we only need a small number of samples to estimate the parameters of the distributions.**\n",
    "\n",
    "***INFO 48:* In machine learning, we often use the second level of abstraction, that is, we fix the parametric form (the univariate Gaussian) and infer the parameters from data.**\n",
    "\n",
    "***INFO 49:* The relationship between the original Bernoulli parameter &#956; and the natural parameter &theta; is known as the sigmoid or logistic function. Observe that &#956; &isin; (0; 1) but &theta; &isin; &real; , and therefore the sigmoid function squeezes a real value into the range (0, 1). This property is useful in machine learning, for example it is used in logistic regression , as well as as a nonlinear activation functions in neural networks.**\n",
    "\n",
    "***INFO 50:* Since machine learning algorithms are implemented on a computer, the mathematical formulations are expressed as numerical optimization methods.**\n",
    "\n",
    "***INFO 51:* Training a machine learning model often boils down to finding a good set of parameters. The notion of “good” is determined by the objective function or the probabilistic model.**\n",
    "\n",
    "***INFO 52:* By convention, most objective functions in machine learning are intended to be minimized, that is, the best value is the minimum value.**\n",
    "\n",
    "***INFO 53:* It turns out that many machine learning objective functions are designed such that they are convex.**\n",
    "\n",
    "***INFO 54:* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
