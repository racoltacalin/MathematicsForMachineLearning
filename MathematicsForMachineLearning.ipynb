{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, vectors are special objects that can be added together and miltiplied by scalars to produce another object of the same king!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Geometric vectors\n",
    "#### 1.2. Polynomials are also vectors\n",
    "#### 1.3. Audio signals are vectors\n",
    "#### 1.4. Elements of R^n (tuples of n real numbers) are vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The concept of a vector space and its properties underline much of ***Machine Learning***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***INFO 1:* Gaussian elimination is an intuitive and constructive way to solve a system of linear equations with thousands of variables.**\n",
    "\n",
    "***INFO 2:* Groups play an important role in computer science.Besides providing a fundamental framework for operations on sets, they are heavily used in\n",
    "cryptography, coding theory, and graphics.**\n",
    "\n",
    "***INFO 3:* Vector subspaces are a key idea in machine learning.**\n",
    "\n",
    "***INFO 4:* Linear independence is one of the most important concepts in linear\n",
    "algebra.**\n",
    "\n",
    "***INFO 5:* In the machine learning literature, the distinction between linear and affine is sometimes not clear so that we can find references to affine spaces/mappings as linear spaces/mappings.**\n",
    "\n",
    "***INFO 6:* Symmetric, positive definite matrices play an important role in machine learning, and they are defined via the inner product. The idea of symmetric positive semidefinite matrices is key in the definition of kernels**\n",
    "\n",
    "***INFO 7:* Projections are an important class of linear transformations (besides rotations and reflections) and play an important role in graphics, coding theory, statistics and machine learning. In machine learning, we often deal with data that is high-dimensional. High-dimensional data is often hard to analyze or visualize. However, high-dimensional data quite often possesses the property that only a few dimensions contain most information, and most other dimensions are not essential to describe key properties of the data. When we compress or visualize high-dimensional data, we will lose information. To minimize this compression loss, we ideally find the most informative dimensions in the data.**\n",
    "\n",
    "***INFO 8:* In machine learning, inner products are important in the context of kernel methods.**\n",
    "\n",
    "***INFO 9:* Methods to analyze and learn from network data are an essential component of machine learning methods.**\n",
    "\n",
    "***INFO 10:* The Cholesky decomposition is an important tool for the numerical computations underlying machine learning.**\n",
    "\n",
    "***INFO 11:* The SVD is used in a variety of applications in machine learning from least-squares problems in curve fitting to solving systems of linear equations. Substituting a matrix with its SVD has often the advantage of making calculation more robust to numerical rounding errors. The SVD’s ability to approximate matrices with “simpler” matrices in a principled manner opens up machine learning applications ranging from dimensionality reduction and topic modeling to data compression and clustering.**\n",
    "\n",
    "***INFO 12:* The low-rank approximation of a matrix appears in many machine learning applications, e.g., image processing, noise filtering, and regularization of ill-posed problems.**\n",
    "\n",
    "***INFO 13:* Many algorithms in machine learning optimize an objective function with respect to a set of desired model parameters that control how well a model explains the data: Finding good parameters can be phrased as an optimization problem.**\n",
    "\n",
    "***INFO 14:* Vector calculus is one of the fundamental mathematical tools we need in machine learning.**\n",
    "\n",
    "***INFO 15:* To facilitate learning in machine learning models, we need to compute gradients of functions, since the gradient points in the direction of steepest ascent.**\n",
    "\n",
    "***INFO 16:* When we compute gradients and implement them, we can use finite differences to numerically test our computation and implementation: We choose the value 'h' to be small (e.g. h = 10^-4) and compare the finite-difference approximation with our -analytic- implementation of the gradient. If the error is small, our gradient implementation is probably correct.**\n",
    "\n",
    "***INFO 17:* The Jacobian determinant is important because we transform random variables and probability distributions. These transformations are extremely relevant in machine learning in the context of *training deep neural networks* using the\n",
    "reparametrization trick, also called infinite perturbation analysis.**\n",
    "\n",
    "***INFO 18:* In many machine learning applications, we find good model parameters by performing gradient descent which relies on the fact that we can compute the gradient of a learning objective with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 19:* For training deep neural network models, the backpropagation algorithm (Kelley, 1960; Bryson, 1961; Dreyfus, 1962; Rumelhart et al., 1986) is an efficient way to compute the gradient of an error function with respect to the parameters of the model.**\n",
    "\n",
    "***INFO 20:* An area where the chain rule is used to an extreme is deep learning, where the function value y is computed as a many-level function composition.**\n",
    "\n",
    "***INFO 21:* It turns out that backpropagation is a special case of a general technique in numerical analysis called automatic differentiation.**\n",
    "\n",
    "***INFO 22:* In the context of neural networks, where the input dimensionality is often much higher than the dimensionality of the labels, the reverse mode is computationally significantly cheaper than the forward mode.**\n",
    "\n",
    "***INFO 23:* For neural network training, we backpropagate the error of the prediction with respect to the label.**\n",
    "\n",
    "***INFO 24:* In machine learning (and other disciplines), we often need to compute expectations, i.e., we need to solve integrals.**\n",
    "\n",
    "***INFO 25:* In machine learning and statistics, there are two major interpretations of probability: the Bayesian and frequentist interpretations.**\n",
    "\n",
    "***INFO 26:* In machine learning, we often avoid explicitly referring to the probability space, but instead refer to probabilities on quantities of interest, which we denote by T. We refer to T as the target space and refer to elements of T as states.**\n",
    "\n",
    "***INFO 27:* In statistics, we observe that something has happened and try to figure out the underlying process that explains the observations. In this sense, machine learning is close to statistics in its goals to construct a model that adequately represents the process that generated the data. We can use the rules of probability to obtain a “best-fitting” model for some\n",
    "data.**\n",
    "\n",
    "***INFO 28:* Another aspect of machine learning systems is that we are interested in generalization error. This means that we are actually interested in the performance of our system on instances that we will observe in future, which are not identical to the instances that we have seen so far.**\n",
    "\n",
    "***INFO 29:* In machine learning, we use discrete probability distributions to model categorical variables, i.e., variables that take a finite set of unordered values. They could be categorical features, such as the degree taken at university when used for predicting the salary of a person, or categorical labels, such as letters of the alphabet when doing handwriting recognition.**\n",
    "\n",
    "***INFO 30:* However, in many machine learning applications discrete states take numerical values, e.g., z1 = -1.1; z2 = 0.3; z3 = 1.5, where we could say z1 < z2 < z3. Discrete states that assume numerical values are particularly useful because we often consider expected values of random variables.**\n",
    "\n",
    "***INFO 31:* Unfortunately, machine learning literature uses notation and nomenclature that hides the distinction between the sample space &#x2126; , the target space T , and the random variable X.** \n",
    "\n",
    "***INFO 32:* In line with most machine learning literature, we also rely on context to distinguish the different uses of the phrase probability distribution.**\n",
    "\n",
    "***INFO 33:* Probabilistic modeling provides a principled foundation for designing machine learning methods.**\n",
    "\n",
    "***INFO 34:* In machine learning and Bayesian statistics, we are often interested in making inferences of unobserved (latent)\\ random variables given that we have observed other random variables.**\n",
    "\n",
    "***INFO 35:* If we think in a bigger context, then the posterior can be used within a decision-making system, and having the full posterior can be extremely useful and lead to decisions that are robust to disturbances.**\n",
    "\n",
    "***INFO 36:* For example, in the context of model-based reinforcement learning, Deisenroth et al. (2015) show that using the full posterior distribution of plausible transition functions leads to very fast (data/sample efficient) learning, whereas focusing on the maximum of the posterior leads to consistent failures. Therefore, having the full posterior can be very useful for a downstream task.**\n",
    "\n",
    "***INFO 37:* The concept of the expected value is central to machine learning, and the foundational concepts of probability itself can be derived from the expected value (Whittle, 2000).**\n",
    "\n",
    "***INFO 38:* In machine learning, we need to learn from empirical observations of data.**\n",
    "\n",
    "***INFO 39:* We use the empirical covariance, which is a biased estimate. The unbiased (sometimes called corrected) covariance has the factor N-1 in the denominator instead of N.**\n",
    "\n",
    "***INFO 40:* The raw-score version of the variance can be useful in machine learning, e.g., when deriving the bias–variance decomposition (Bishop, 2006).**\n",
    "\n",
    "***INFO 41:* In machine learning, we often consider problems that can be modeled as independent and identically distributed (i.i.d.) random variables, X_1,...,X_N.**\n",
    "\n",
    "***INFO 42:* Another concept that is important in machine learning is conditional independence.**\n",
    "\n",
    "***INFO 43:* There are many other areas of machine learning that also benefit from using a Gaussian distribution, for example Gaussian processes, variational inference, and reinforcement learning.**\n",
    "\n",
    "***INFO 44:* Gaussians are widely used in statistical estimation and machine learning as they have closed-form expressions for marginal and conditional distributions.**\n",
    "\n",
    "***INFO 45:* It is worth recalling at this point the desiderata for manipulating probability distributions in the machine learning context: *1.* There is some “closure property” when applying the rules of probability, e.g., Bayes’ theorem. By closure, we mean that applying a particular operation returns an object of the same type. *2.* As we collect more data, we do not need more parameters to describe the distribution. *3.* Since we are interested in learning from data, we want parameter estimation to behave nicely.**\n",
    "\n",
    "***INFO 46:* The rewriting above of the Bernoulli distribution, where we use Boolean variables as numerical 0 or 1 and express them in the exponents, is a trick that is often used in machine learning textbooks.**\n",
    "\n",
    "***INFO 47:* In machine learning, we consider a finite number of samples from a distribution. One could imagine that for simple distributions we only need a small number of samples to estimate the parameters of the distributions.**\n",
    "\n",
    "***INFO 48:* In machine learning, we often use the second level of abstraction, that is, we fix the parametric form (the univariate Gaussian) and infer the parameters from data.**\n",
    "\n",
    "***INFO 49:* The relationship between the original Bernoulli parameter &#956; and the natural parameter &theta; is known as the sigmoid or logistic function. Observe that &#956; &isin; (0; 1) but &theta; &isin; &real; , and therefore the sigmoid function squeezes a real value into the range (0, 1). This property is useful in machine learning, for example it is used in logistic regression , as well as as a nonlinear activation functions in neural networks.**\n",
    "\n",
    "***INFO 50:* Since machine learning algorithms are implemented on a computer, the mathematical formulations are expressed as numerical optimization methods.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Optimization \n",
    "\n",
    "***INFO 1:* Training a machine learning model often boils down to finding a good set of parameters. The notion of “good” is determined by the objective function or the probabilistic model.**\n",
    "\n",
    "***INFO 2:* By convention, most objective functions in machine learning are intended to be minimized, that is, the best value is the minimum value.**\n",
    "\n",
    "***INFO 3:* It turns out that many machine learning objective functions are designed such that they are convex.**\n",
    "\n",
    "***INFO 4:* Gradient descent is a first-order optimization algorithm.**\n",
    "\n",
    "***INFO 5:* The step-size is also called the learning rate.**\n",
    "\n",
    "***INFO 6:* Standard gradient descent, as introduced previously, is a “batch” optimization method, i.e., optimization is performed using the full training set by updating the vector of parameters.**\n",
    "\n",
    "***INFO 7:* When the learning rate decreases at an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to local minimum (Bottou, 1998).** \n",
    "\n",
    "***INFO 8:* In machine learning, optimization methods are used for training by minimizing an objective function on the training data, but the overall goal is to improve generalization performance.**\n",
    "\n",
    "***INFO 9:* Since the goal in machine learning does not necessarily need a precise estimate of the minimum of the objective function, approximate gradients using mini-batch approaches have been widely used.**\n",
    "\n",
    "***INFO 10:* Stochastic gradient descent is very effective in large-scale machine learning problems such as training deep neural networks on millions of images, topic models, reinforcement learning, or training of large-scale Gaussian process models**\n",
    "\n",
    "***INFO 11:* In machine learning, we often use sums of functions; for example, the objective function of the training set includes a sum of the losses for each example in the training set.**\n",
    "\n",
    "***INFO 12:* The Legendre-Fenchel conjugate turns out to be quite useful for machine learning problems that can be expressed as convex optimization problems. In particular, for convex loss functions that apply independently to each example, the conjugate loss is a convenient way to derive a dual problem.**\n",
    "\n",
    "***INFO 13:* Modern applications of machine learning often mean that the size of datasets prohibit the use of batch gradient descent, and hence stochastic gradient descent is the current workhorse of large-scale machine learning methods.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Central Machine Learning Problems\n",
    "### 8.0. When Models Meet Data\n",
    "#### Four pillars of Machine Learning:\n",
    "1. REGRESSION\n",
    "\n",
    "2. Dimensionality reduction\n",
    "\n",
    "3. Density estimation\n",
    "\n",
    "4. Classification\n",
    "\n",
    "## 8.1. Data, Models, and Learning \n",
    "\n",
    "***INFO 1:* There are three major components of a machine learning system: data, models, and learning. The main question of machine learning is “What do we mean by good models?”.**\n",
    " \n",
    "## 8.1.1. Data as Vectors\n",
    "\n",
    "***INFO 2:* Data is assumed to be in a tidy format. Each row of the table as representing a particular instance or example, and each column to be a particular feature.**\n",
    "\n",
    "***INFO 3:* Even numerical data that could potentially be directly read into a machine learning algorithm should be carefully considered for units, scaling, and constraints.**\n",
    "\n",
    "***INFO 4:* Each row is a particular individual x_n, often referred to as an example or data point in machine learning.Each column represents a particular feature of interest about the example, and we index the features as d = 1,....,D.**\n",
    "\n",
    "***INFO 5:* In many machine learning algorithms, we need to additionally be able to compare two vectors.**\n",
    "\n",
    "***INFO 6:* In recent years, deep learning methods have shown promise in using the data itself to learn new good features\n",
    "and have been very successful in areas, such as computer vision, speech recognition, and natural language processing.**\n",
    "\n",
    "### 8.1.2. Models as Functions\n",
    "\n",
    "***INFO 7:* A predictor is a function that, when given a particular input example produces an output**\n",
    "\n",
    "***INFO 8:* Linear functions strike a good balance between the generality of the problems that can be solved and the amount of background mathematics that is needed.**\n",
    "\n",
    "### 8.1.3 Models as Probability Distributions\n",
    "\n",
    "***INFO 9:* Instead of considering a predictor as a single function, we could consider predictors to be probabilistic models, i.e., models describing the distribution of possible functions.**\n",
    "\n",
    "### 8.1.4 Learning is Finding Parameters\n",
    "\n",
    "***INFO 10:* The goal of learning is to find a model and its corresponding parameters such that the resulting predictor will perform well on unseen data.**\n",
    "\n",
    "***INFO 11:* There are conceptually three distinct algorithmic phases when discussing machine learning algorithms:            1. *Prediction or inference* || *2. Training or parameter estimation* || *3. Hyperparameter tuning or model selection***\n",
    "\n",
    "***INFO 12:* We are interested in learning a model based on data such that it performs well on future data. It is not enough for the model to only fit the training data well, the predictor needs to perform well on unseen data.**\n",
    "\n",
    "***INFO 13:* The distinction between parameters and hyperparameters is somewhat arbitrary, and is mostly driven by the distinction between what can be numerically optimized versus what needs to use search techniques.**\n",
    "\n",
    "\n",
    "## 8.2 Empirical Risk Minimization\n",
    "\n",
    "***INFO 14:* The “learning” part of machine learning boils down to estimating parameters based on training data.**\n",
    "\n",
    "### 8.2.1 Hypothesis Class of Functions \n",
    "\n",
    "***INFO 15:* It is also common in machine learning to choose a parametrized class of functions, for example affine functions.**\n",
    "\n",
    "***INFO 16:* Recent advances in neural networks allow for efficient computation of more complex non-linear function classes.**\n",
    "\n",
    "### 8.2.2 Loss Function for Training\n",
    "\n",
    "***INFO 17:* We are not interested in a predictor that only performs well on the training data. Instead, we seek a predictor that performs well (has low risk) on unseen test data.**\n",
    "\n",
    "***INFO 18:* Many machine learning tasks are specified with an associated performance measure, e.g., accuracy of prediction or root mean squared error.**\n",
    "\n",
    "### 8.2.3 Regularization to Reduce Overfitting\n",
    "\n",
    "***INFO 19:* We simulate this unseen data by holding out a proportion of the whole dataset.**\n",
    "\n",
    "***INFO 20:* It is important for the user to not cycle back to a new round of training after having observed the test set.**\n",
    "\n",
    "***INFO 21:* In machine learning, the penalty term is referred to as regularization. Regularization is a way to compromise between accurate solution of empirical risk minimization and the size or complexity of the solution.**\n",
    "\n",
    "### 8.2.4 Cross-Validation to Assess the Generalization Performance\n",
    "\n",
    "***INFO 22:* Cross-validation iterates through (ideally) all combinations of assignments of chunks to R and V (validation set)w.**\n",
    "\n",
    "***INFO 23:* Evaluating the quality of the model, depending on these hyperparameters, may result in a number of training runs that is exponential in the number of model parameters.**\n",
    "\n",
    "***INFO 24:* However, cross-validation is an embarrassingly parallel problem, i.e., little effort is needed to separate the  problem into a number of parallel tasks. Given sufficient computing resources (e.g., cloud computing, server farms), cross validation does not require longer than a single performance assessment.**\n",
    "\n",
    "***INFO 25:* In this section, we saw that empirical risk minimization is based on the following concepts: the hypothesis class of functions, the loss function and regularization.**\n",
    "\n",
    "***INFO 26:* A recent machine theory learning textbook that builds on the theoretical foundations and develops efficient learning algorithms is Shalev-Shwartz and Ben-David (2014).**\n",
    "\n",
    "***INFO 27:* An alternative to cross-validation is bootstrap and jackknife (Efron and Tibshirani, 1993; Davidson and Hinkley, 1997; Hall, 1992).**\n",
    "\n",
    "***INFO 28:* Thinking about empirical risk minimization as “probability free” is incorrect.**\n",
    "\n",
    "## 8.3 Parameter Estimation\n",
    "\n",
    "### 8.3.1 Maximum Likelihood Estimation\n",
    "\n",
    "***INFO 29:* The idea behind maximum likelihood estimation (MLE) is to define a function of the parameters that enables us to find a model that fits the data well.**\n",
    "\n",
    "***INFO 30:* It is often easier from an optimization viewpoint to compute functions that can be decomposed into sums of simpler functions.**\n",
    "\n",
    "***INFO 31:* It turns out that for Gaussian likelihoods the resulting optimization problem corresponding to maximum likelihood estimation has a closed-form solution.**\n",
    "\n",
    "### 8.3.2 Maximum A Posteriori Estimation\n",
    "\n",
    "***INFO 32:* Note that the conjugate prior of a Gaussian is also a Gaussian and therefore we expect the posterior distribution to also be a Gaussian.**\n",
    "\n",
    "***INFO 33:* The idea of including prior knowledge about where good parameters lie is widespread in machine learning.**\n",
    "\n",
    "***INFO 34:* The principle of maximum likelihood estimation (and maximum a posteriori estimation) uses probabilistic modeling to reason about the uncertainty in the data and model parameters.**\n",
    "\n",
    "### 8.3.3 Model Fitting\n",
    "\n",
    "***INFO 35:* When we talk about “fitting”, we typically mean optimizing/learning model parameters so that they minimize some loss function, e.g., the negative log-likelihood.**\n",
    "\n",
    "***INFO 36:* With maximum likelihood and maximum a posteriori estimation, two commonly used algorithms for model fitting.**\n",
    "\n",
    "***INFO 37:* Models that overfit typically have a large number of parameters.**\n",
    "\n",
    "***INFO 38:* One way to detect overfitting in practice is to observe that the model has low training risk but high test risk during cross validation.**\n",
    "\n",
    "***INFO 39:* Models that underfit typically have few parameters.**\n",
    "\n",
    "***INFO 40:* In practice, we often define very rich model classes M_&theta; with many parameters, such as deep neural networks.**\n",
    "\n",
    "***INFO 41:* To mitigate the problem of overfitting, we can use regularization or priors.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.4 Probabilistic Modeling and Inference\n",
    "\n",
    "***INFO 1:* In machine learning, we are frequently concerned with the interpretation and analysis of data, e.g., for prediction of future events and decision making. To make this task more tractable, we often build models that describe the generative process that generates the observed data.**\n",
    "\n",
    "## 8.4.1 Probabilistic Models\n",
    "\n",
    "***INFO 2:* A probabilistic model is specified by the joint distribution of all random variables.**\n",
    "\n",
    "***INFO 3:* The benefit of using probabilistic models is that they offer a unified and consistent set of tools from probability theory for modeling, inference, prediction, and model selection.**\n",
    "\n",
    "## 8.4.2 Bayesian Inference\n",
    "\n",
    "***INFO 4:* Parameter estimation can be phrased as an optimization problem.**\n",
    "\n",
    "***INFO 5:* A key task in machine learning is to take a model and the data to uncover the values of the model’s hidden variables &theta; given the observed variables x.**\n",
    "\n",
    "***INFO 6:* Focusing solely on some statistic of the posterior distribution leads to loss of information, which can be critical in a system that uses the prediction p(x | &theta;*) to make decisions.**\n",
    "\n",
    "***INFO 7:* Bayesian inference is about learning the distribution of random variables.**\n",
    "\n",
    "***INFO 8:* Therefore, having the full posterior distribution around can be extremely useful and leads to more robust decisions. Bayesian inference is about finding this posterior distribution.**\n",
    "\n",
    "***INFO 9:* Bayesian inference inverts the relationship between parameters and the data.**\n",
    "\n",
    "***INFO 10:* The implication of having a posterior distribution on the parameters is that it can be used to propagate uncertainty from the parameters to the data.**\n",
    "\n",
    "***INFO 11:* The prediction is an average over all plausible parameter values &theta;, where the plausibility is encapsulated by the parameter distribution p(&theta;).**\n",
    "\n",
    "***INFO 12:* Parameter estimation via maximum likelihood or MAP estimation yields a consistent point estimate &theta;* of the parameters, and the key computational problem to be solved is optimization. Bayesian inference yields a (posterior) distribution, and the key computational problem to be solved is integration.**\n",
    "\n",
    "***INFO 13:* Predictions with point estimates are straightforward, whereas predictions in the Bayesian framework require solving another integration problem.**\n",
    "\n",
    "***INFO 14:* Bayesian inference gives us a principled way to incorporate prior knowledge, account for side information, and incorporate structural knowledge, all of which is not easily done in the context of parameter estimation.**\n",
    "\n",
    "***INFO 15:* The propagation of parameter uncertainty to the prediction can be valuable in decision-making systems for risk assessment and exploration in the context of data-efficient learning.**\n",
    "\n",
    "***INFO 16:* While Bayesian inference is a mathematically principled framework for learning about parameters and making predictions, there are some practical challenges that come with it because of the integration problems we need to solve.**\n",
    "\n",
    "***INFO 17:* There are generic tools, such as Bayesian optimization that are very useful ingredients for an efficient search of meta parameters of models or algorithms.**\n",
    "\n",
    "***INFO 18:* In the machine learning literature, there can be a somewhat arbitrary separation between (random) “variables” and “parameters”. While parameters are estimated (e.g., via maximum likelihood), variables are usually marginalized out.**\n",
    "\n",
    "## 8.4.3 Latent-Variable Models\n",
    "\n",
    "***INFO 19:* In practice, it is sometimes useful to have additional latent variables z (besides the model parameters &theta;) as part of the model. These latent variables are different from the model parameters &theta; as they do not parametrize the model explicitly.**\n",
    "\n",
    "***INFO 20:* Latent variables often simplify the structure of the model and allow us to define simpler and richer model structures. Simplification of the model structure often goes hand in hand with a smaller number of model parameters.**\n",
    "\n",
    "***INFO 21:* Learning in latent-variable models (at least via maximum likelihood) can be done in a principled way using the expectation maximization (EM) algorithm.**\n",
    "\n",
    "***INFO 22:* The likelihood is a function of the data and the model parameters, but is independent of the latent variables.**\n",
    "\n",
    "***INFO 23:* We can exploit the fact that all the elements of a probabilistic model are random variables to define a unified language for representing them.**\n",
    "\n",
    "## 8.4.4 Further Reading\n",
    "\n",
    "***INFO 24:* Probabilistic models in machine learning provide a way for users to capture uncertainty about data and predictive models in a principled fashion. **\n",
    "\n",
    "***INFO 25:* In general, analytic solutions are rare, and computational methods such as sampling and variational inference are used.**\n",
    "\n",
    "***INFO 26:* In recent years, several programming languages have been proposed that aim to treat the variables defined in software as random variables corresponding to probability distributions. The objective is to be able to write complex functions of probability distributions, while under the hood the compiler automatically takes care of the rules of Bayesian inference.**\n",
    "\n",
    "# 8.5 Directed Graphical Models\n",
    "\n",
    "***INFO 27:* Directed graphical models are also known as Bayesian networks.**\n",
    "\n",
    "### 8.5.1 Graph Semantics\n",
    "\n",
    "***INFO 28:* The graph layout depends on the factorization of the joint distribution.**\n",
    "\n",
    "***INFO 29:* Statistical independence means that the distribution factorizes.**\n",
    "\n",
    "***INFO 30:* A hyperprior is a second layer of prior distributions on the parameters of the first layer of priors.**\n",
    "\n",
    "### 8.5.2 Conditional Independence and d-Separation\n",
    "\n",
    "***INFO 31:* Directed graphical models allow a compact representation of probabilistic models.**\n",
    "\n",
    "***INFO 32:* These modeling assumptions (hyperparameters) affect the prediction performance, but cannot be selected directly using the approaches we have seen so far.**\n",
    "\n",
    "### 8.5.3 Further Reading\n",
    "\n",
    "***INFO 33:* Graphical models allow for graph-based algorithms for inference and learning, e.g., via local message passing. Applications range from ranking in online games and computer vision (e.g., image segmentation, semantic labeling, image denoising, image restoration (Kittler and F¨oglein, 1984; Sucar and Gillies, 1994; Shotton et al., 2006; Szeliski et al., 2008)) to coding theory, solving linear equation systems, and iterative Bayesian state estimation in signal processing.**\n",
    "\n",
    "***INFO 34:* One topic that is particularly important in real applications that we do not discuss in this book is the idea of structured prediction which allows machine learning models to tackle predictions that are structured, for example sequences, trees, and graphs.**\n",
    "\n",
    "***INFO 35:* The popularity of neural network models has allowed more flexible probabilistic models to be used, resulting in many useful applications of structured models.**\n",
    "\n",
    "# 8.6 Model Selection\n",
    "\n",
    "***INFO 36:* In machine learning, we often need to make high-level modeling decisions that critically influence the performance of the model.**\n",
    "\n",
    "***INFO 37:* More complex models are more flexible in the sense that they can be used to describe more datasets.**\n",
    "\n",
    "***INFO 38:* One would now think that very flexible models are generally preferable to simple models because they are more expressive.**\n",
    "\n",
    "***INFO 39:* A general problem is that at training time we can only use the training set to evaluate the performance of the model and learn its parameters.**\n",
    "\n",
    "***INFO 40:* The performance on the training set is not really what we are interested in.**\n",
    "\n",
    "***INFO 41:* The maximum likelihood estimation can lead to overfitting, especially when the training dataset is small.**\n",
    "\n",
    "***INFO 42:* The model works well on the test set (which is not available at training time) it is ideally.**\n",
    "\n",
    "## 8.6.1 Nested Cross-Validation\n",
    "\n",
    "***INFO 43:* The cross-validation provides an estimate of the generalization error by repeatedly splitting the dataset into training and validation sets.**\n",
    "\n",
    "***INFO 44:* We can test different model and hyperparameter choices in the inner loop.**\n",
    "\n",
    "***INFO 45:* To distinguish the two levels, the set used to estimate the generalization performance is often called the test set and the set used for choosing the best model is called the validation set.**\n",
    "\n",
    "## 8.6.2 Bayesian Model Selection\n",
    "\n",
    "***INFO 46:* **\n",
    "\n",
    "***INFO 47:* **\n",
    "\n",
    "***INFO 48:* **\n",
    "\n",
    "***INFO 49:* **\n",
    "\n",
    "***INFO 50:* **\n",
    "\n",
    "***INFO 51:* **\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
